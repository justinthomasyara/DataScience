{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='top'></a>\n",
    "\n",
    "# Homework 2: Data Visualization and Probability Analysis \n",
    "***\n",
    "\n",
    "**Name**: Justin Yara\n",
    "\n",
    "***\n",
    "\n",
    "This assignment is due on Canvas by **MIDNIGHT on Monday September 14**. Your solutions to theoretical questions should be done in Markdown directly below the associated question.  Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  Remember that you are encouraged to discuss the problems with your classmates, but **you must write all code and solutions on your own**.\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Any relevant data sets should be available under the **Data** module on Canvas. To make life easier on the graders if they need to run your code, do not change the relative path names here. Instead, move the files around on your computer.\n",
    "- If you're not familiar with typesetting math directly into Markdown then by all means, do your work on paper first and then typeset it later.  Remember that there is a [reference guide](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference) linked on Canvas on writing math in Markdown. **All** of your written commentary, justifications and mathematical work should be in Markdown.\n",
    "- Because you can technically evaluate notebook cells is a non-linear order, it's a good idea to do Kernel $\\rightarrow$ Restart & Run All as a check before submitting your solutions.  That way if we need to run your code you will know that it will work as expected. \n",
    "- It is **bad form** to make your reader interpret numerical output from your code.  If a question asks you to compute some value from the data you should show your code output **AND** write a summary of the results in Markdown directly below your code. \n",
    "- 95 points of this assignment are in problems.  The remaining 5 are for neatness, style, and overall exposition of both code and text.\n",
    "- This probably goes without saying, but... For any question that asks you to calculate something, you **must show all work and justify your answers to receive credit**. Sparse or nonexistent work will receive sparse or nonexistent credit. \n",
    "\n",
    "---\n",
    "**Shortcuts:**  [Problem 1](#p1) | [Problem 2](#p2) | [Problem 3](#p3) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)\n",
    "\n",
    "<br>\n",
    "\n",
    "<a id='p1'></a>\n",
    "\n",
    "## (15 points) Problem 1: Theory (Median Distance)\n",
    "***\n",
    "\n",
    "One way we conceptualize many data science questions is asking for the \"best choice\" of some parameter on data set.  We should be able to justify that our measures of centrality should in some way be the \"best\" ways to represent the data.\n",
    "\n",
    "\n",
    "In lecture, we may have discussed the following important property of the mean:\n",
    "\n",
    "\n",
    "The *sample mean* of data $X_1, X_2, \\dots X_n$ is the unique minimizer $c$ of the function $$f(c)=\\sum_{i=1}^n \\left(X_i-c \\right)^2. $$\n",
    "\n",
    "The proof of that claim is as follows:\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "Differentiating yields\n",
    "$$f'(c)=\\frac{df}{dc}\\sum_{i=1}^n \\left(X_i-c \\right)^2 =\\sum_{i=1}^n-2(X_i-c).$$ \n",
    "\n",
    "Setting $f'(c)=0$ gives\n",
    "\n",
    "$$0=\\sum_{i=1}^n-2(X_i-c)$$\n",
    "$$=2nc-2\\sum_{i=1}^n X_i$$\n",
    "$$\\implies\\qquad  c=\\frac{\\sum_{i=1}^n X_i}{n}=\\bar{X}$$\n",
    "\n",
    "***\n",
    "\n",
    "### Your exercise:\n",
    "\n",
    "You are tasked with recreating a *similar* proof.  Prove the following:\n",
    "\n",
    "The *median* of data $X_1, X_2, \\dots X_n$ is the possibly non-unique minimizer $c$ of the function $$f(c)=\\sum_{i=1}^n |X_i-c| $$\n",
    "\n",
    "A few things to think about:\n",
    "\n",
    " - how do we differentiate the absolute value function?\n",
    " - what conditions might make the median non-unique in this case?  If it's nonunique, what possible values of $c$ still minimize the function $f$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "The general format of the proof is the same as above, but now we have to differential the absolute value function.  Recall the definition of absolute value:\n",
    "\n",
    "\n",
    "$$\n",
    "|X_i-c| = \\left\\{\n",
    "    \\begin{array}\\\\\n",
    "        X_i-c & \\mbox{if } \\ X_i-c \\geq 0 \\\\\n",
    "        -(X_i-c) & \\mbox{if } X_i-c < 0 \\\\\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "If we're differentiating this, then, we have:\n",
    "$$\n",
    "\\frac{d}{dc}|X_i-c| = \\left\\{\n",
    "    \\begin{array}\\\\\n",
    "        -1 & \\mbox{if } \\ X_i-c \\geq 0 \\\\\n",
    "        1 & \\mbox{if } X_i-c < 0 \\\\\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Now we replicate the proof from above. Consider:\n",
    "\n",
    "$$f(c)=\\sum_{i=1}^n |X_i-c| $$\n",
    "\n",
    "Differentiating yields\n",
    "$$f'(c)=\\frac{df}{dc}\\sum_{i=1}^n |X_i-c| =\\sum_{i=1}^n  K_i$$\n",
    "where \n",
    "$$\n",
    "K_i = \\left\\{\n",
    "    \\begin{array}\\\\\n",
    "        -1 & \\mbox{if } \\ X_i-c \\geq 0 \\\\\n",
    "        1 & \\mbox{if } X_i-c < 0 \\\\\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Setting $f'(c)=0$ means we need to choose $c$ so that this sum is zero.  Since the sum is composed of entirely +1s and -1s, we need to choose $c$ so that there are equal numbers of +1s and -1s.  Thus:\n",
    "\n",
    "The sum is minimized for all choices of $c$ with equally many data points $X_i>c$ as data points $X_i<c$.\n",
    "\n",
    "If the $X_i$ values are sorted, it's the middle of the $X_i$ list: that's how we have defined the median, previously!\n",
    "\n",
    "A couple of minor details:\n",
    "\n",
    " - The absolute value function is not actually differentiable at the values when $X_i=c$, since this would be a cusp.  This doesn't matter here, however, since we've trained to check those values for local min/max anyways.\n",
    " - If we have an odd number of data points, we must choose $c$ to be the exact \"middle\" point.  If this point is a *repeated* measurement, we should verify that choosing the middle observation still works: e.g. for data `X=[1,1,2]`, choosing a median of 1 is still the best choice.\n",
    " - If we have an even number of data points, e.g. for `X=[1,2]`, the distance-minimizer is *any* point between-and-including 1 or 2.  Choosing the average between those points is a statistical convention but not necessarily \"optimal\" my our data science measure!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)\n",
    "<a id='p2'></a>\n",
    "\n",
    "## (40 pts) Problem 2: Computation (Streaming Means)\n",
    "***\n",
    "\n",
    "Data science is often divided into two categories: questions of *what* the best value might be to repreesnt a data problem, and questions of *how* to compute that data value.  Question 1 - and prior lectures - should tell you that computing the mean is valuable!  But *how* do we compute the mean?\n",
    "\n",
    "Let $x_1, x_2, \\ldots, x_n$ be $n$ observations of a variable of interest.  Recall that the sample mean $\\bar{x}_n$ and sample variance $s^2_n$ are given by \n",
    "<a id='eq1'></a>\n",
    "$$\n",
    "\\bar{x}_n = \\frac{1}{n}\\sum_{k=1}^n x_k \\quad \\textrm{and} \\quad s^2_n = \\frac{1}{n-1}\\sum_{k=1}^n \\left( x_k - \\bar{x}_n\\right)^2 \\qquad \\tag{Equation 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**:\n",
    "\n",
    "How many computations - floating point operations: addition, subtraction, multiplication, division each count as 1 operation - are required to compute the mean of the data set with $n$ observations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "Computing a mean with $n$ observations takes $n-1$ additions followed by a division, for $n$ operations total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**:\n",
    "\n",
    "Now suppose our data is *streaming*- we slowly add observations one at a time, instead of seeing the entire data set at once.  We are still interested in the mean, so if we stream the data set `[4,6,0,10, ...]`, we first compute the mean of the the first data point `[4]`, then we recompute the mean of the first two points `[4,6]`, then we recompute the mean of three `[4,6,0]`, and so forth.\n",
    "\n",
    "Suppose we recompute the mean from scratch after each and every one of our $n$ observations are one-by-one added to our data set.  How many floating point operations are spent computing (and re-computing) the mean of the data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    " This means the first mean takes 1, the second mean takes 2, the third mean takes 3, and so forth.  Once we're computed $n$ sequential means, the total amount of computations is:\n",
    "\n",
    "$1+2+3+\\dots + n= \\sum_{i=1}^n i = \\frac{n(n+1)}{2}$ computations.  This is *order* $n^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be convinced that streaming a mean costs a lot more computer time than just computing once!\n",
    "\n",
    "In this problem we explore a smarter method for such an _online_ computation of the mean.  \n",
    "\n",
    "**Result**: The following relation holds between the mean of the first $n-1$ observations and the mean of all $n$ observations: \n",
    "\n",
    "$$\n",
    "\\bar{x}_n = \\bar{x}_{n-1} + \\frac{x_n - \\bar{x}_{n-1}}{n}\n",
    "$$\n",
    "\n",
    "\n",
    "A proof of this result is in the [Appendix](#Appendix) after problem 3, and requires some careful manipulations of the sum $\\bar{x}_n$.  Your task will be to computationally verify and utilize this result.\n",
    "\n",
    "**Part C**: Write a function `my_sample_mean` that takes as its input a numpy array and returns the mean of that numpy array using the formulas from class ([Equation 1](#eq1)). Write another function `my_sample_var` that takes as its input a numpy array and returns the variance of that numpy array, again using the formulas from class ([Equation 1](#eq1)). You may **not** use any built-in sample mean or variance functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution:\n",
    "\n",
    "def my_sample_mean(xx):\n",
    "    n = len(xx)\n",
    "    total = np.sum(xx)\n",
    "    return total/n\n",
    "\n",
    "def my_sample_var(xx):\n",
    "    mean = my_sample_mean(xx)\n",
    "    centered = xx - mean\n",
    "    squared = centered**2\n",
    "    return np.sum(squared)/(len(xx)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Use your functions from Part B to compute the sample mean and sample variance of the following array, which contains the minutes late that the BuffBus is running on Friday afternoon.\n",
    "\n",
    "`bus = [312, 4, 10, 0, 22, 39, 81, 19, 8, 60, 80, 42]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample mean is:  56.416666666666664 ; sample variance is:  7274.628787878787\n"
     ]
    }
   ],
   "source": [
    "bus = [312, 4, 10, 0, 22, 39, 81, 19, 8, 60, 80, 42]\n",
    "mean = my_sample_mean(bus)\n",
    "var = my_sample_var(bus)\n",
    "print(\"Sample mean is: \", mean,\"; sample variance is: \", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: Implement a third function called `update_mean` that implements the formula discussed after part B. Note that this function will need to take as its input three things: $x_n$, $\\bar{x}_{n-1}$ and $n$, and returns $\\bar{x}_{n}$. A function header and return statement are provided for you. This function may be auto-graded, so please do not change the given header API - the order of inputs matters! If you change it, you might lose points.\n",
    "\n",
    "Use this function to compute the values that you get from taking the mean of the first buff buses' lateness, the first two buff buses' lateness, the first three buff buses' lateness, and so on up to all of the `bus` data points from **Part D**. Store your streaming bus means in a numpy array called `buffbus_bad_means`.  Report all 12 estimates in `buffbus_bad_means`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 1 busses have a mean lateness of 312.000 minutes\n",
      "The first 2 busses have a mean lateness of 158.000 minutes\n",
      "The first 3 busses have a mean lateness of 108.667 minutes\n",
      "The first 4 busses have a mean lateness of 81.500 minutes\n",
      "The first 5 busses have a mean lateness of 69.600 minutes\n",
      "The first 6 busses have a mean lateness of 64.500 minutes\n",
      "The first 7 busses have a mean lateness of 66.857 minutes\n",
      "The first 8 busses have a mean lateness of 60.875 minutes\n",
      "The first 9 busses have a mean lateness of 55.000 minutes\n",
      "The first 10 busses have a mean lateness of 55.500 minutes\n",
      "The first 11 busses have a mean lateness of 57.727 minutes\n",
      "The first 12 busses have a mean lateness of 56.417 minutes\n"
     ]
    }
   ],
   "source": [
    "# Given API:\n",
    "def update_mean(prev_mean, xn, n):\n",
    "    new_mean = prev_mean + (xn - prev_mean)/n\n",
    "    return(new_mean)\n",
    "\n",
    "buffbus_bad_means = []\n",
    "\n",
    "prior_mean = bus[0]\n",
    "\n",
    "length_of_loop = np.size(bus)\n",
    "for i in range(length_of_loop):\n",
    "    prior_mean = update_mean(prior_mean,bus[i],i+1)\n",
    "    buffbus_bad_means.append(prior_mean)\n",
    "buffbus_bad_means = np.array(buffbus_bad_means)\n",
    "\n",
    "for idx,times in enumerate(buffbus_bad_means):\n",
    "    print(\"The first {:.0f} busses have a mean lateness of {:.3f} minutes\".format(idx+1,times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You may report any results for part E here, if not done using print() statements**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure your function complies with the given API, run this small test, where we suppose we have a mean of $\\bar{x}_n = 1$ with the first $2$ data points (`prev_mean`), and we update this with the 3rd ($n=3$) data point which is $x_3=2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert update_mean(1,2,3)==4/3, \"Warning: function seems broken.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part F**:\n",
    "\n",
    "How many floating point operations were spent computing the final result in your code in **part E**?  Is this truly better than the uninformed approach from **part B**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "The update_mean function took only 3 operations, and we perform it for all data points after the first ($n-1$ times total).  This means that for a data set that streams in $n$ data point, we preform $3(n-1)$ operations compared to the naive approach taking $\\frac{n(n+1)}{2}$ operations.  This is an *order* $n$ operation instead of $n^2$, so the larger the data gets, the more time we save!\n",
    "\n",
    "On our example with only 12 data points, the naive approach would perform 78 operations and the update_mean function just 33, so we're already saving plenty of time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)\n",
    "\n",
    "<a id='p3'></a>\n",
    "\n",
    "## (40 pts) Problem 3: Data (Probability and Histograms)\n",
    "*** \n",
    "The sinking of the RMS Titanic was a terrible tragedy that saw the loss of many lives. Even within this tragedy, thanks to the combinations of the records of the White Star Line and the thorough nature of follow-up research after the accident we have some records that can help us try to piece together the course of events on board the ship. Many of the historians and other researchers who have investigated this event have speculated as to what exactly happened.\n",
    "\n",
    "We have the data on survival rates by class, gender, and age, so let's figure out whether there is evidence for some of these scenarios. Access the Titanic data in `titanic_data.csv` and store it in a Pandas DataFrame. The data contains information pertaining to class status (**Pclass**), survival (**Survived**), and gender (**Sex**) of passengers, among other things. Be sure to use the `titanic_data.csv` data set, *not* the `clean_titanic_data` file or `dirty_titanic_data` file from the in-class notebook exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File titanic_data.csv does not exist: 'titanic_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-95f212589725>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'titanic_data.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(df.keys())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File titanic_data.csv does not exist: 'titanic_data.csv'"
     ]
    }
   ],
   "source": [
    "filepath = 'titanic_data.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "df.head()\n",
    "# print(df.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**:\n",
    "Based on the overall population of passengers, report the probability of survival.\n",
    "\n",
    "$$P(Survived=1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Solution: \" + str(df[\"Survived\"].sum()/df[\"Survived\"].count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: \n",
    "Some claim that the final hours aboard the RMS Titanic were marked by \"class warfare\" in which the people with first-class tickets took all the good spots on the lifeboats; others claim that the final hours were characterized by male chivalry, in which the men valiantly gave up their positions in the boats and succumbed bravely to the depths of the Atlantic. \n",
    "\n",
    "Consider the two claims: class warfare, and male chivalry. Suppose that class warfare occurred in the final hours aboard the Titanic.  What patterns might you expect to see in the data?  Suppose that male chivalry was widespread during the final hours instead. What patterns might you then expect to see in the data?  Explain both of these hypothesized patterns in words. Are these two hypotheses mutually exclusive or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION**\n",
    "\n",
    "If there were class warfare, we would expect that the first class is asserting its privilege over the lower classes, so first class passengers should be more likely to survive.  If there were chivalry, it would be the case that men have lower survival rates than women.  These are not mutually exclusive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Use Pandas methods to create a clean data set by removing any rows from the DataFrame that are missing values corresponding to **Survived**, **Pclass**, **Age**, or **Sex**. Store the clean data in a DataFrame called dfTitanic. Be sure to show any exploratory work determining if/where there are rows with missing values. _HINT: There should be 714 rows in your cleaned data set._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTitanic = df.dropna(subset=[\"Survived\", \"Pclass\", \"Age\", \"Sex\"])\n",
    "#dfTitanic.head()\n",
    "dfTitanic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Compute the probability of survival according to class, gender, and all combinations of the two variables.  Then, answer the following questions:\n",
    "* **(i)** When reviewing class survival probability, how do the results compare to the base survival probability results from **Part A**?\n",
    "* **(ii)** When reviewing gender survival probability, how do the results compare to the base survival probability results from **Part A**?\n",
    "* **(iii)** Within each passenger class, were men or women more/less/equally likely to survive?\n",
    "* **(iv)**  Did men in first class or women in third class have a higher survival probability?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SOLUTION\")\n",
    "\n",
    "for sex in [\"male\", \"female\"]:\n",
    "    survived = dfTitanic.loc[(dfTitanic[\"Sex\"]==sex), \"Survived\"].sum()\n",
    "    total = len(dfTitanic.loc[(dfTitanic[\"Sex\"]==sex), \"Survived\"])\n",
    "    print(\"{:>6} Survived: {:.0f}, Total: {:.0f}, Survival Fraction: {:.3f}\".format(sex, survived, total, survived/total))\n",
    "\n",
    "for pc in [1,2,3]:\n",
    "    survived = dfTitanic.loc[(dfTitanic[\"Pclass\"]==pc), \"Survived\"].sum()\n",
    "    total = len(dfTitanic.loc[(dfTitanic[\"Pclass\"]==pc), \"Survived\"])\n",
    "    #print(pc, sex, survived, total)\n",
    "    print(\"Class {} Survived: {:.0f}, Total: {:.0f}, Survival Fraction: {:.3f}\".format(pc, survived, total, survived/total))\n",
    "        \n",
    "for pc in [1,2,3]:\n",
    "    for sex in [\"male\", \"female\"]:\n",
    "        survived = dfTitanic.loc[(dfTitanic[\"Pclass\"]==pc) & (dfTitanic[\"Sex\"]==sex), \"Survived\"].sum()\n",
    "        total = len(dfTitanic.loc[(dfTitanic[\"Pclass\"]==pc) & (dfTitanic[\"Sex\"]==sex), \"Survived\"])\n",
    "        #print(pc, sex, survived, total)\n",
    "        print(\"Class {} {:>6} Survived: {:.0f}, Total: {:.0f}, Survival Fraction: {:.3f}\".format(pc, sex, survived, total, survived/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each class, women had more probability of survival compared to men. Women in third class had a higher probability of survival."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: One might wonder how a passenger's age is related to the likelihood that they would survive the Titanic disaster. In addition to the \"male chivalry\" argument outlined above, you can perhaps imagine an addendum - \"women and children first!\" - as the cry to ring out across the decks. Or you might imagine the opposite - rather than \"class warfare\", it is simply healthy adults fighting to take lifeboat spots for themselves.\n",
    "\n",
    "To answer this question graphically, plot two density histograms on the same set of axes, showing the distribution of the ages of passengers who survived, and the distribution of the ages of passengers who did not. \n",
    "* Use the bin edges $[0,5,10,\\ldots,70,75,80]$ for both histograms.\n",
    "* To better distinguish between our populations, we will represent survivors with `navy` (as they were eventually rescued by ships) and those who passed away with `sandybrown`.\n",
    "* Plot both histograms on a single set of axes (there should be only one panel in the figure you create), but use Matplotlib/Pandas plotting functionality to make the faces of the histogram boxes somewhat transparent, so both histograms are visible.\n",
    "* Include a legend and label your axes.\n",
    "* Comment on the results. Does your figure suggest that some age ranges are more or less likely to have survived the disaster than other ages? Fully explain your reasoning and use your figure to justify your conclusions.\n",
    "* If you noticed some relationship between age and likelihood of survival, what is one possible explanation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bins = range(0,80,5)\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "dfTitanic.loc[dfTitanic[\"Survived\"]==1].hist(column=\"Age\", ax=ax, facecolor=\"navy\", edgecolor=\"white\",bins=my_bins, alpha=0.5, label=\"Survived\", density=True)\n",
    "dfTitanic.loc[dfTitanic[\"Survived\"]==0].hist(column=\"Age\", ax=ax, facecolor=\"sandybrown\", edgecolor=\"white\",bins=my_bins, alpha=0.3, label=\"Died\", density=True)\n",
    "ax.set_xlabel(\"Age\", fontsize=16)\n",
    "ax.set_ylabel(\"Frequency\", fontsize=16)\n",
    "ax.set_title(\"Passenger Age\", fontsize=24)\n",
    "ax.grid(alpha=0.25)\n",
    "ax.set_axisbelow(True)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The density histogram for passengers who died is taller (i.e., more likely) for passengers under the age of 15 and passengers in their 30s. So it appears children and adults in their 30s were more likely to perish, whereas young adults (15-30) seem to be more likely than not to survive. For adults over the age of 40, there does not seem to be any discernable trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part F:** In Part E, we plotted two *density* histograms, showing the distributions of ages of passengers that survived or did not survive the Titanic disaster. Why would it be misleading for us to have plotted these as *frequency* histograms instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "It would be misleading to plot these as frequency histograms because there are many more passengers who did not survive the disaster (about 2/3 of them), so the frequency histogram for that data subset would have more area than the frequency histogram for the surviving passengers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part G**: Do the data suggest class warfare, male chivalry, age bias, or some combination of these characteristics in the final hours aboard the Titanic?  Justify your conclusions based on the computations done above, or do any other analysis that you like, but be sure to clearly justify your conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXAMPLE**\n",
    "Women tended to survive more than men, so it appears that the men were being chivalrous and making sure that the women got off the boat first. On the other hand, the first class folks were far more likely to survive. We also saw some sort of age bias in survival for the \"young adults\", which could be due to a conscious attempt or it could be due to a simple healthiness effect (children and older folks may not have been able to handle the arctic waters). It could be a mix of all of these factors. However, it is also entirely possible that we need more information. For example, perhaps the person's location on the ship would be a good item to study in relation to survival."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P.S.** It is not a component of your graded assignment, but the 1997 James Cameron film _Titanic_ captured some of these very notions in some riveting cinema. Whether or not you found evidence for these cases in the data, you might find it interesting to watch the movie (or just the segments of the sinking) to see one interpretation of these ideas. You can perhaps see how we might be persuaded to reinterpret the evidence of data by a heart-wrenching performance from a handsome young Leonardo DiCaprio!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Appendix'></a>\n",
    "\n",
    "## Appendix \n",
    "\n",
    "*Goal*: Prove that \n",
    "$$\n",
    "\\bar{x}_n = \\bar{x}_{n-1} + \\frac{x_n - \\bar{x}_{n-1}}{n}\n",
    "$$\n",
    "\n",
    "Note that you can get an expression for $\\bar{x}_{n-1}$ by simply replacing $n$ in Equation 1 above with $n-1$.\n",
    "\n",
    "We'll start with $\\bar{x}_n$ and massage it until we get the righthand side of the formula\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\nonumber \\bar{x}_n &=& \\frac{1}{n} \\sum_{k=1}^n x_k \\\\\n",
    "&=& \\frac{1}{n} \\sum_{k=1}^{n-1} x_k + \\frac{1}{n}x_n \\\\\n",
    "&=& \\frac{n-1}{n-1}\\frac{1}{n} \\sum_{k=1}^{n-1} x_k + \\frac{1}{n}x_n \\\\\n",
    "&=& \\frac{n-1}{n} \\left(\\frac{1}{n-1} \\sum_{k=1}^{n-1} x_k\\right) + \\frac{1}{n}x_n \\\\\n",
    "&=& \\frac{n-1}{n} \\bar{x}_{n-1} + \\frac{1}{n}x_n \\\\\n",
    "&=& \\frac{n}{n}\\bar{x}_{n-1} - \\frac{1}{n}\\bar{x}_{n-1} + \\frac{1}{n}x_n \\\\\n",
    "&=&  \\bar{x}_{n-1} + \\frac{x_n - \\bar{x}_{n-1}}{n} \\quad \\checkmark\n",
    "\\end{eqnarray}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
